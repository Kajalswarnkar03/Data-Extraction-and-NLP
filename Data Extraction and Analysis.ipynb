{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e07ee6bc-69ab-44f9-959f-dbbe2443f3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing library\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.request import urlopen as uReq\n",
    "import requests\n",
    "import urllib\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lem=WordNetLemmatizer()\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "import string\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b76b2c4-8a78-4b21-8f58-d1ea21085e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing input file\n",
    "df=pd.read_csv('input1.csv')[['URL_ID','URL']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e39152d4-a7bc-4b85-9c82-76510c7ae2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.iloc[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "538feafa-2068-41a3-a33d-9cb36f5bf267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>blackassign0096</td>\n",
       "      <td>https://insights.blackcoffer.com/what-is-the-r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>blackassign0097</td>\n",
       "      <td>https://insights.blackcoffer.com/impact-of-cov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>blackassign0098</td>\n",
       "      <td>https://insights.blackcoffer.com/contribution-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>blackassign0099</td>\n",
       "      <td>https://insights.blackcoffer.com/how-covid-19-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>blackassign0100</td>\n",
       "      <td>https://insights.blackcoffer.com/how-will-covi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             URL_ID                                                URL\n",
       "0   blackassign0001  https://insights.blackcoffer.com/rising-it-cit...\n",
       "1   blackassign0002  https://insights.blackcoffer.com/rising-it-cit...\n",
       "2   blackassign0003  https://insights.blackcoffer.com/internet-dema...\n",
       "3   blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...\n",
       "4   blackassign0005  https://insights.blackcoffer.com/ott-platform-...\n",
       "..              ...                                                ...\n",
       "95  blackassign0096  https://insights.blackcoffer.com/what-is-the-r...\n",
       "96  blackassign0097  https://insights.blackcoffer.com/impact-of-cov...\n",
       "97  blackassign0098  https://insights.blackcoffer.com/contribution-...\n",
       "98  blackassign0099  https://insights.blackcoffer.com/how-covid-19-...\n",
       "99  blackassign0100  https://insights.blackcoffer.com/how-will-covi...\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ade5e24-3ffc-49b9-af22-c28fa7ff178e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('URL_ID',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c9fd82-f209-49b6-99a4-ddd2572e0774",
   "metadata": {},
   "source": [
    "# Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3db5505-fee4-486d-bd85-1a51885fa3e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import pandas as pd\\nimport requests\\nfrom bs4 import BeautifulSoup\\n\\n# Ensure you have a DataFrame df with your URLs\\n# For example:\\n# df = pd.DataFrame({\\'urls\\': [\\'http://example.com/page1\\', \\'http://example.com/page2\\']})\\n\\nurl_id = 1\\nfor i in range(len(df)):\\n    j = df.iloc[i].values\\n\\n    headers = {\\n        \\'User-Agent\\': \\'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36\\'\\n    }\\n    \\n    # Load text from URL\\n    page = requests.get(j[0], headers=headers)\\n    \\n    # Parse the page content\\n    soup = BeautifulSoup(page.content, \\'html.parser\\')\\n    \\n    # Extract text content\\n    content = soup.findAll(attrs={\\'class\\': \\'td-post-content\\'})\\n    if content:\\n        content_text = content[0].text.replace(\\'\\xa0\\', \" \").replace(\\'\\n\\', \" \")\\n    else:\\n        content_text = \\'\\'\\n    \\n    # Extract title\\n    title = soup.findAll(attrs={\\'class\\': \\'entry-title\\'})\\n    if title:\\n        title_text = title[0].text.replace(\\'\\n\\', \" \").replace(\\'/\\', \"\")\\n    else:\\n        title_text = \\'\\'\\n    \\n    # Merge title and content text\\n    text = title_text + \\'.\\' + content_text\\n    \\n    # Save the text to a file\\n    filename = f\"{url_id}.txt\"\\n    with open(filename, \\'w\\', encoding=\\'utf-8\\') as file:\\n        file.write(text)\\n    \\n    url_id += 1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Ensure you have a DataFrame df with your URLs\n",
    "# For example:\n",
    "# df = pd.DataFrame({'urls': ['http://example.com/page1', 'http://example.com/page2']})\n",
    "\n",
    "url_id = 1\n",
    "for i in range(len(df)):\n",
    "    j = df.iloc[i].values\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    # Load text from URL\n",
    "    page = requests.get(j[0], headers=headers)\n",
    "    \n",
    "    # Parse the page content\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    # Extract text content\n",
    "    content = soup.findAll(attrs={'class': 'td-post-content'})\n",
    "    if content:\n",
    "        content_text = content[0].text.replace('\\xa0', \" \").replace('\\n', \" \")\n",
    "    else:\n",
    "        content_text = ''\n",
    "    \n",
    "    # Extract title\n",
    "    title = soup.findAll(attrs={'class': 'entry-title'})\n",
    "    if title:\n",
    "        title_text = title[0].text.replace('\\n', \" \").replace('/', \"\")\n",
    "    else:\n",
    "        title_text = ''\n",
    "    \n",
    "    # Merge title and content text\n",
    "    text = title_text + '.' + content_text\n",
    "    \n",
    "    # Save the text to a file\n",
    "    filename = f\"{url_id}.txt\"\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(text)\n",
    "    \n",
    "    url_id += 1'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d6cdcd-3466-4301-a0e3-7002102b84c3",
   "metadata": {},
   "source": [
    "# Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38a0b96a-11ba-4580-9d6b-7ec3c559a63f",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Read the Extracted Text Data\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize an empty list to store the text data\n",
    "text_data = []\n",
    "\n",
    "# Assuming you have saved files named 1.txt, 2.txt, ..., you can read them\n",
    "num_files = 100  # Adjust based on the number of files you have\n",
    "for i in range(1, num_files + 1):\n",
    "    filename = f\"{i}.txt\"\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        text_data.append({'id': i, 'text': text})\n",
    "\n",
    "# Create a DataFrame from the collected text data\n",
    "df_text = pd.DataFrame(text_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "becf5f54-ebf5-4f23-a404-a0f9cd1ebaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9268ecb0-99b2-4f7a-b662-3898dff52a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Textual Analysis and Variable Computation\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Download NLTK resources if not already downloaded\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "# Function to compute variables for each text\n",
    "def compute_variables(text):\n",
    "    # Tokenization and word count\n",
    "    tokens = word_tokenize(text)\n",
    "    word_count = len(tokens)\n",
    "    \n",
    "    # Stopword count\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stopwords_count = sum(1 for word in tokens if word.lower() in stop_words)\n",
    "    \n",
    "    # Average word length\n",
    "    if word_count > 0:\n",
    "        avg_word_length = sum(len(word) for word in tokens) / word_count\n",
    "    else:\n",
    "        avg_word_length = 0\n",
    "    \n",
    "    # Sentiment analysis using TextBlob\n",
    "    blob = TextBlob(text)\n",
    "    sentiment_polarity = blob.sentiment.polarity\n",
    "    sentiment_subjectivity = blob.sentiment.subjectivity\n",
    "    \n",
    "    return {\n",
    "        'Word Count': word_count,\n",
    "        'Stopword Count': stopwords_count,\n",
    "        'Average Word Length': avg_word_length,\n",
    "        'Sentiment Polarity': sentiment_polarity,\n",
    "        'Sentiment Subjectivity': sentiment_subjectivity\n",
    "    }\n",
    "\n",
    "# Compute variables for each text in df_text\n",
    "df_text['Variables'] = df_text['text'].apply(compute_variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "902a6d6d-785e-4ebc-82c4-6733affcb514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Blank_link={}\\nclass DataIngestion:\\n    \\n    def secondary(self):\\n        data=pd.read_excel(\\'C:/Users/RANU RAJA/Desktop/Blockcoffer project/Input.xlsx\\')\\n        df=data.copy() #create a copy to avoid modifying the original DataFrame\\n\\n        # Create an empty \\'article_words\\' column\\n        df[\\'article_words\\']=\\'\\'\\n\\n        for i, url in enumerate(df[\\'URL\\']):\\n            response_code=requests.get(url)\\n            soup=bs(response_code.text, \\'html.parser\\')\\n            article_title=soup.find(\\'title\\').text\\n\\n            all_text_element=soup.find(\"div\", class_=\"td-post-content tagdiv-type\")\\n            \\n            if all_text_element is not None:\\n                all_text=all_text_element.get_text(strip=True, separator=\\'\\n\\')\\n                firstdata=all_text.splitlines()\\n            else:\\n                print(f\"No matching element found in the HTML for URL: {url}\")\\n                Blank_link[f\"blackassign00{i+1}\"]=url\\n                firstdata=[]\\n                for i, j in Blank_link.items():\\n                    if url==j:\\n                        response_code=requests.get(j)\\n                        soup=bs(response_code.text,\\'html.parser\\')\\n                        article_title=soup.find(\\'title\\').text\\n                        alldiv=soup.find(\"div\", class_=\"td_block_wrap tdb_single_content tdi_130 td-pb-border-top td_block_template_1 td-post-content tagdiv-type\")\\n                        firstdata=alldiv.text\\n                        return firstdata\\n                else:\\n                       print(f\"No mathching element found in the HTML for URL: {url}\")\\n\\n            filename=urllib.parse.quote_plus(url)\\n            file_path=\\'C:/Users/RANU RAJA/Desktop/Blockcoffer project\\'\\n            space=\" \"\\n\\n            with open(f\"{file_path}\\\\{filename}.txt\", \\'w+\\') as file1:\\n                file1.writelines(article_title)\\n                file1.writelines(space)\\n                file1.writelines(firstdata)\\n\\n            # Update \\'article_words\\' column for the current row \\n            df.at[i, \\'article_words\\']=f\"{article_title}-{firstdata}\"\\n\\n        df.to_csv(\\'C:/Users/RANU RAJA/Desktop/Blockcoffer project/Input.csv\\', index=False)\\n        return df\\n\\nif __name__ == \"__main__\":\\n    obj = DataIngestion()\\n    obj.secondary()'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Blank_link={}\n",
    "class DataIngestion:\n",
    "    \n",
    "    def secondary(self):\n",
    "        data=pd.read_excel('C:/Users/RANU RAJA/Desktop/Blockcoffer project/Input.xlsx')\n",
    "        df=data.copy() #create a copy to avoid modifying the original DataFrame\n",
    "\n",
    "        # Create an empty 'article_words' column\n",
    "        df['article_words']=''\n",
    "\n",
    "        for i, url in enumerate(df['URL']):\n",
    "            response_code=requests.get(url)\n",
    "            soup=bs(response_code.text, 'html.parser')\n",
    "            article_title=soup.find('title').text\n",
    "\n",
    "            all_text_element=soup.find(\"div\", class_=\"td-post-content tagdiv-type\")\n",
    "            \n",
    "            if all_text_element is not None:\n",
    "                all_text=all_text_element.get_text(strip=True, separator='\\n')\n",
    "                firstdata=all_text.splitlines()\n",
    "            else:\n",
    "                print(f\"No matching element found in the HTML for URL: {url}\")\n",
    "                Blank_link[f\"blackassign00{i+1}\"]=url\n",
    "                firstdata=[]\n",
    "                for i, j in Blank_link.items():\n",
    "                    if url==j:\n",
    "                        response_code=requests.get(j)\n",
    "                        soup=bs(response_code.text,'html.parser')\n",
    "                        article_title=soup.find('title').text\n",
    "                        alldiv=soup.find(\"div\", class_=\"td_block_wrap tdb_single_content tdi_130 td-pb-border-top td_block_template_1 td-post-content tagdiv-type\")\n",
    "                        firstdata=alldiv.text\n",
    "                        return firstdata\n",
    "                else:\n",
    "                       print(f\"No mathching element found in the HTML for URL: {url}\")\n",
    "\n",
    "            filename=urllib.parse.quote_plus(url)\n",
    "            file_path='C:/Users/RANU RAJA/Desktop/Blockcoffer project'\n",
    "            space=\" \"\n",
    "\n",
    "            with open(f\"{file_path}\\{filename}.txt\", 'w+') as file1:\n",
    "                file1.writelines(article_title)\n",
    "                file1.writelines(space)\n",
    "                file1.writelines(firstdata)\n",
    "\n",
    "            # Update 'article_words' column for the current row \n",
    "            df.at[i, 'article_words']=f\"{article_title}-{firstdata}\"\n",
    "\n",
    "        df.to_csv('C:/Users/RANU RAJA/Desktop/Blockcoffer project/Input.csv', index=False)\n",
    "        return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    obj = DataIngestion()\n",
    "    obj.secondary()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb7a5123-6929-4352-ad76-cd19aad4aa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a200056f-9555-4595-984a-75a3f84db2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Blank_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d055eba-a6a1-440b-86cf-107528d3e393",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_path='C:/Users/RANU RAJA/Desktop/Blockcoffer internship project'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2d632c9-842e-48ec-9cfc-c4c8d27df643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'updated_list=[]\\n\\nfor i, j in Blank_link.items():\\n    response_code=requests.get(j)\\n    soup=bs(response_code.text, \\'html.parser\\')\\n    article_title=soup.find(\\'title\\').text\\n\\n    alldiv=soup.find(\"div\", class_=\"td_block_wrap tdb_single_content tdi_130 td-pb-border-top td_block_template_1 td-post-content tagdiv-type\")\\n\\n    if alldiv is not None:\\n        firstdata=alldiv.text\\n        filename=urllib.parse.quote_plus(j)\\n        filepath=\\'C:/Users/RANU RAJA/Desktop/Blockcoffer internship project\\'\\n        space=\" \"\\n\\n        with open(f\"{file_path}\\\\{filename}.txt\", \\'w+\\') as file1:\\n                file1.writelines(article_title)\\n                file1.writelines(space)\\n                file1.writelines(firstdata)\\n        updated_dict={\\n            \\'URL_ID\\':i,\\n            \\'URL\\':j,\\n            \\'article_words\\':f\"{article_title} - {firstdata}\"\\n        }\\n        updated_list.append(updated_dict)\\n    else:\\n        print(f\"No data available for the link: {j}\")\\nupdated_df=pd.DataFrame(updated_list)'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''updated_list=[]\n",
    "\n",
    "for i, j in Blank_link.items():\n",
    "    response_code=requests.get(j)\n",
    "    soup=bs(response_code.text, 'html.parser')\n",
    "    article_title=soup.find('title').text\n",
    "\n",
    "    alldiv=soup.find(\"div\", class_=\"td_block_wrap tdb_single_content tdi_130 td-pb-border-top td_block_template_1 td-post-content tagdiv-type\")\n",
    "\n",
    "    if alldiv is not None:\n",
    "        firstdata=alldiv.text\n",
    "        filename=urllib.parse.quote_plus(j)\n",
    "        filepath='C:/Users/RANU RAJA/Desktop/Blockcoffer internship project'\n",
    "        space=\" \"\n",
    "\n",
    "        with open(f\"{file_path}\\{filename}.txt\", 'w+') as file1:\n",
    "                file1.writelines(article_title)\n",
    "                file1.writelines(space)\n",
    "                file1.writelines(firstdata)\n",
    "        updated_dict={\n",
    "            'URL_ID':i,\n",
    "            'URL':j,\n",
    "            'article_words':f\"{article_title} - {firstdata}\"\n",
    "        }\n",
    "        updated_list.append(updated_dict)\n",
    "    else:\n",
    "        print(f\"No data available for the link: {j}\")\n",
    "updated_df=pd.DataFrame(updated_list)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ce534e2-7bca-42ef-91e7-dc1b77d4a1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remain_data=pd.DataFrame(updated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9476c1dd-5175-4eab-9334-527fdec16568",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remain_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5657265c-27db-4000-b641-593894ef5029",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df=pd.merge(df, updated_df[['URL', 'article_words']], on='URL', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "691a4078-f34b-405a-b316-82e6a7ca8adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25c6a607-1f79-437f-ab4c-5ed86ebb1076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again try to perform analysis on data and performing operations\n",
    "text=pd.read_csv('C:/Users/RANU RAJA/Desktop/Blockcoffer internship project/100.txt',header=None)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eda83e75-7766-4ad9-b10f-0dee6611f9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1 entries, 0 to 0\n",
      "Data columns (total 75 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   0       1 non-null      object\n",
      " 1   1       1 non-null      object\n",
      " 2   2       1 non-null      object\n",
      " 3   3       1 non-null      object\n",
      " 4   4       1 non-null      object\n",
      " 5   5       1 non-null      object\n",
      " 6   6       1 non-null      object\n",
      " 7   7       1 non-null      object\n",
      " 8   8       1 non-null      object\n",
      " 9   9       1 non-null      object\n",
      " 10  10      1 non-null      object\n",
      " 11  11      1 non-null      object\n",
      " 12  12      1 non-null      object\n",
      " 13  13      1 non-null      object\n",
      " 14  14      1 non-null      object\n",
      " 15  15      1 non-null      object\n",
      " 16  16      1 non-null      object\n",
      " 17  17      1 non-null      object\n",
      " 18  18      1 non-null      object\n",
      " 19  19      1 non-null      object\n",
      " 20  20      1 non-null      object\n",
      " 21  21      1 non-null      object\n",
      " 22  22      1 non-null      object\n",
      " 23  23      1 non-null      object\n",
      " 24  24      1 non-null      object\n",
      " 25  25      1 non-null      object\n",
      " 26  26      1 non-null      object\n",
      " 27  27      1 non-null      object\n",
      " 28  28      1 non-null      object\n",
      " 29  29      1 non-null      object\n",
      " 30  30      1 non-null      object\n",
      " 31  31      1 non-null      object\n",
      " 32  32      1 non-null      object\n",
      " 33  33      1 non-null      object\n",
      " 34  34      1 non-null      object\n",
      " 35  35      1 non-null      object\n",
      " 36  36      1 non-null      object\n",
      " 37  37      1 non-null      object\n",
      " 38  38      1 non-null      object\n",
      " 39  39      1 non-null      object\n",
      " 40  40      1 non-null      object\n",
      " 41  41      1 non-null      object\n",
      " 42  42      1 non-null      object\n",
      " 43  43      1 non-null      object\n",
      " 44  44      1 non-null      object\n",
      " 45  45      1 non-null      object\n",
      " 46  46      1 non-null      object\n",
      " 47  47      1 non-null      object\n",
      " 48  48      1 non-null      object\n",
      " 49  49      1 non-null      object\n",
      " 50  50      1 non-null      object\n",
      " 51  51      1 non-null      object\n",
      " 52  52      1 non-null      object\n",
      " 53  53      1 non-null      object\n",
      " 54  54      1 non-null      object\n",
      " 55  55      1 non-null      object\n",
      " 56  56      1 non-null      object\n",
      " 57  57      1 non-null      object\n",
      " 58  58      1 non-null      object\n",
      " 59  59      1 non-null      object\n",
      " 60  60      1 non-null      object\n",
      " 61  61      1 non-null      object\n",
      " 62  62      1 non-null      object\n",
      " 63  63      1 non-null      object\n",
      " 64  64      1 non-null      object\n",
      " 65  65      1 non-null      object\n",
      " 66  66      1 non-null      object\n",
      " 67  67      1 non-null      object\n",
      " 68  68      1 non-null      object\n",
      " 69  69      1 non-null      object\n",
      " 70  70      1 non-null      object\n",
      " 71  71      1 non-null      object\n",
      " 72  72      1 non-null      object\n",
      " 73  73      1 non-null      object\n",
      " 74  74      1 non-null      object\n",
      "dtypes: object(75)\n",
      "memory usage: 728.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "#information of data frame\n",
    "text.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4d0a975-21c8-41e2-a514-c91f40be389a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing extra created column\n",
    "text.drop(1,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1c198d6-e632-4257-abfd-cddf325102cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting type\n",
    "text=text.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96a56db2-2977-40b0-aeaf-89ef333da0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting text to sentence\n",
    "import re\n",
    "a=text[0].str.split('([\\.]\\s)',expand=False)#splitting text on '.'\n",
    "b=a.explode()#converting to rows\n",
    "b=pd.DataFrame(b)#creating data frame\n",
    "b.columns=['abc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "246bbdb6-f0e7-49c9-8c31-20ed19bb8901",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing . char from each rows\n",
    "def abcd(x):    \n",
    "    nopunc =[char for char in x if char != '.']\n",
    "    return ''.join(nopunc)\n",
    "b['abc']=b['abc'].apply(abcd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64742e22-c26a-43b1-b6ce-a880e7418066",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing emty space with null values\n",
    "c=b.replace('',np.nan,regex=True)\n",
    "c=c.mask(c==\" \")\n",
    "c=c.dropna()\n",
    "c.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3cc29d0-437e-40ea-9f76-a79e3f369035",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing nltk library and stopwords\n",
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b47fa0d-08e5-4122-bdb8-1c3d4dfa25ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punc=[punc for punc in string.punctuation]\n",
    "punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a03c1db6-75f0-4b84-a877-1cbaf90b33fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing stop words files that are provided\n",
    "StopWords_Auditor=open('C:/Users/RANU RAJA/Desktop/Blockcoffer internship project/StopWords/StopWords_Auditor.txt','r',encoding='ISO-8859-1')\n",
    "StopWords_Currencies=open('C:/Users/RANU RAJA/Desktop/Blockcoffer internship project/StopWords/StopWords_Currencies.txt','r',encoding='ISO-8859-1')\n",
    "StopWords_DatesandNumbers=open('C:/Users/RANU RAJA/Desktop/Blockcoffer internship project/StopWords/StopWords_DatesandNumbers.txt','r',encoding='ISO-8859-1')\n",
    "StopWords_Generic=open('C:/Users/RANU RAJA/Desktop/Blockcoffer internship project/StopWords/StopWords_Generic.txt','r',encoding='ISO-8859-1')\n",
    "StopWords_GenericLong=open('C:/Users/RANU RAJA/Desktop/Blockcoffer internship project/StopWords/StopWords_GenericLong.txt','r',encoding='ISO-8859-1')\n",
    "StopWords_Geographic=open('C:/Users/RANU RAJA/Desktop/Blockcoffer internship project/StopWords/StopWords_Geographic.txt','r',encoding='ISO-8859-1')\n",
    "StopWords_Names=open('C:/Users/RANU RAJA/Desktop/Blockcoffer internship project/StopWords/StopWords_Names.txt','r',encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0e49a49-5671-4fe5-b93c-07969d9575f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ERNST\\n',\n",
       " 'YOUNG\\n',\n",
       " 'DELOITTE\\n',\n",
       " 'TOUCHE\\n',\n",
       " 'KPMG\\n',\n",
       " 'PRICEWATERHOUSECOOPERS\\n',\n",
       " 'PRICEWATERHOUSE\\n',\n",
       " 'COOPERS\\n']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StopWords_Auditor.seek(0)\n",
    "StopWords_Auditor.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "68c10f6d-2845-48db-b553-c9757570e47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating func for removing stop words\n",
    "def text_process(text):\n",
    "    nopunc =[char for char in text if char not in punc or char not in [':',',','(',')','’','?']]\n",
    "    nopunc=''.join(nopunc)\n",
    "    txt=' '.join([word for word in nopunc.split() if word.lower() not in StopWords_Auditor])\n",
    "    txt1=' '.join([word for word in txt.split() if word.lower() not in StopWords_Currencies])\n",
    "    txt2=' '.join([word for word in txt1.split() if word.lower() not in StopWords_DatesandNumbers])\n",
    "    txt3=' '.join([word for word in txt2.split() if word.lower() not in StopWords_Generic])\n",
    "    txt4=' '.join([word for word in txt3.split() if word.lower() not in StopWords_GenericLong])\n",
    "    txt5=' '.join([word for word in txt4.split() if word.lower() not in StopWords_Geographic])\n",
    "    return ' '.join([word for word in txt5.split() if word.lower() not in StopWords_Names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2d557d4-bafa-4422-8ab5-f4df20bed505",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying func for each row\n",
    "c['abc']=c['abc'].apply(text_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379074d9-3cac-4da3-9e2d-e6bd05040e1d",
   "metadata": {},
   "source": [
    "# Variables\r\n",
    "The definition of each of the variables given in the “Text Analysis.docx” file\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "23aee1da-3843-49e3-b1f9-54b242c646da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Ensure necessary NLTK data packages are downloaded\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Load stopwords and sentiment word lists\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "positive= set(open('C:/Users/RANU RAJA/Desktop/Blockcoffer internship project/MasterDictionary/positive-words.txt', 'r').read().splitlines())\n",
    "negative= set(open('C:/Users/RANU RAJA/Desktop/Blockcoffer internship project/MasterDictionary/negative-words.txt', 'r').read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1be1cae7-a7d2-47c2-af0b-4f1931ad78a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of creating pandas DataFrames from existing data\n",
    "positive = pd.DataFrame(positive, columns=['abc'])\n",
    "negative = pd.DataFrame(negative, columns=['abc'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a425966f-6e2e-47b5-9786-572561bada99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns to 'abc'\n",
    "positive.columns = ['abc']\n",
    "negative.columns = ['abc']\n",
    "\n",
    "# Convert the 'abc' column to string type\n",
    "positive['abc'] = positive['abc'].astype(str)\n",
    "negative['abc'] = negative['abc'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d4c3ceff-fbb9-40ec-8758-6558830b7c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#positive list\n",
    "length=positive.shape[0]\n",
    "post=[]\n",
    "for i in range(0,length):\n",
    "   nopunc =[char for char in positive.iloc[i] if char not in string.punctuation or char != '+']\n",
    "   nopunc=''.join(nopunc)\n",
    "\n",
    "   post.append(nopunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d02c48c0-e130-4c05-a291-fa22ba10c84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#negative list\n",
    "length=negative.shape[0]\n",
    "neg=[]\n",
    "for i in range(0,length):\n",
    "  nopunc =[char for char in negative.iloc[i] if char not in string.punctuation or char != '+']\n",
    "  nopunc=''.join(nopunc)\n",
    "  neg.append(nopunc)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1ea7eed9-88c3-4b91-954d-ed603274d299",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing tokenize library\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "33fae2f2-5a0d-45c8-9bfb-3b8a4a65875e",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_list=[]\n",
    "length=c.shape[0]\n",
    "for i in range(0,length):\n",
    "  txt=' '.join([word for word in c.iloc[i]])\n",
    "  txt_list.append(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "eaab98d4-4fd2-48c5-85be-1797f577e89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization of text\n",
    "tokenize_text=[]\n",
    "for i in txt_list:\n",
    "  \n",
    "  tokenize_text+=(word_tokenize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "a6122af7-c547-4e9d-bb72-2446cb6f0e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Efficient', 'Supply', 'Chain', 'Assessment', 'Overcoming', 'Technical', 'Hurdles', 'for', 'Web', 'Application', 'Development', 'As', 'business', 'close', 'to', 'help', 'prevent', 'transmission', 'of', 'COVID-19']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "db19ee5e-b408-4e09-89e4-cd660cc64bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596db77d-ae7d-42af-ace8-2bbffe770179",
   "metadata": {},
   "source": [
    "# 1) POSITIVE SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "2f7a0823-9ac0-40a5-9014-d8b39713b5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postive score= 1\n"
     ]
    }
   ],
   "source": [
    "positive_score=0\n",
    "for i in tokenize_text:\n",
    "  if(i.lower() in post):\n",
    "    positive_score+=1\n",
    "print('postive score=', positive_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3663adf3-5f5b-40df-aea3-133d32b741f1",
   "metadata": {},
   "source": [
    "# 2)NEGATIVE SCORE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "92860bf5-2ce7-40ab-bfb5-a0ef39d7bde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative score= 0\n"
     ]
    }
   ],
   "source": [
    "negative_score=0\n",
    "for i in tokenize_text:\n",
    "  if(i.lower() in neg):\n",
    "    negative_score+=1\n",
    "print('negative score=', negative_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462f089b-5d27-4bfd-bf17-cb1613a9cf89",
   "metadata": {},
   "source": [
    "# 3) POLARITY SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "22226913-bb05-49d9-9e49-63c73fbd0f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polarity_score= 0.9999990000010001\n"
     ]
    }
   ],
   "source": [
    "#Polarity Score = (Positive Score – Negative Score)/ ((Positive Score + Negative Score) + 0.000001)\n",
    "Polarity_Score=(positive_score-negative_score)/((positive_score+negative_score)+0.000001)\n",
    "print('polarity_score=', Polarity_Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55360a9e-4f1a-43c5-b55b-204b95d53660",
   "metadata": {},
   "source": [
    "# 4) SUBJECTIVITY SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "781688d8-49ef-4672-8d20-0747b7ac99d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subjectivity_score 0.04999999750000012\n"
     ]
    }
   ],
   "source": [
    "#Subjectivity Score = (Positive Score + Negative Score)/ ((Total Words after cleaning) + 0.000001)\n",
    "subjectiivity_score=(positive_score-negative_score)/((len(tokenize_text))+ 0.000001)\n",
    "print('subjectivity_score',subjectiivity_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bce790-af89-439a-a25d-94b0f2847d60",
   "metadata": {},
   "source": [
    "# 5) AVG SENTENCE LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "80d8c556-5efc-4c65-9e3a-63056b95a7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg sentence length= 76.0\n"
     ]
    }
   ],
   "source": [
    "length=c.shape[0]\n",
    "avg_length=[]\n",
    "for i in range(0,length):\n",
    "  avg_length.append(len(c['abc'].iloc[i]))\n",
    "avg_senetence_length=sum(avg_length)/len(avg_length)\n",
    "print('avg sentence length=', avg_senetence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b52d9a-2823-4819-9187-a4c0da77e515",
   "metadata": {},
   "source": [
    "# 6) PERCENTAGE OF COMPLEX WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "37192a9a-4ac8-404a-933e-75cd90f04a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "vowels=['a','e','i','o','u']\n",
    "import re\n",
    "count=0\n",
    "complex_Word_Count=0\n",
    "for i in tokenize_text:\n",
    "  x=re.compile('[es|ed]$')\n",
    "  if x.match(i.lower()):\n",
    "   count+=0\n",
    "  else:\n",
    "    for j in i:\n",
    "      if(j.lower() in vowels ):\n",
    "        count+=1\n",
    "  if(count>2):\n",
    "   complex_Word_Count+=1\n",
    "  count=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "91733a01-f390-45e9-b920-9ec9fc64d8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentag of complex words=  0.4\n"
     ]
    }
   ],
   "source": [
    "Percentage_of_Complex_words=complex_Word_Count/len(tokenize_text)\n",
    "print('percentag of complex words= ',Percentage_of_Complex_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b51a96-57b8-4afd-8cfd-838fd68d32a3",
   "metadata": {},
   "source": [
    "# 7) FOG INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "4326e8b5-f8d0-439d-99ff-6b19914543ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fog index=  30.560000000000002\n"
     ]
    }
   ],
   "source": [
    "#Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)\n",
    "Fog_Index = 0.4 * (avg_senetence_length + Percentage_of_Complex_words)\n",
    "print('fog index= ',Fog_Index )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece5e77e-4873-4ef7-a75e-f2630eea0aa8",
   "metadata": {},
   "source": [
    "# 8) AVG NUMBER OF WORDS PER SENTENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "837a70fc-4148-4369-923c-bd99a4907d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg no of words per sentence=  10.0\n"
     ]
    }
   ],
   "source": [
    "length=c.shape[0]\n",
    "avg_length=[]\n",
    "for i in range(0,length):\n",
    "  a=[word.split( ) for word in c.iloc[i]]\n",
    "  avg_length.append(len(a[0]))\n",
    "  a=0\n",
    "#avg\n",
    "avg_no_of_words_per_sentence=sum(avg_length)/length\n",
    "print(\"avg no of words per sentence= \",avg_no_of_words_per_sentence)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c099f5-6cc8-4441-8333-366777b09e0b",
   "metadata": {},
   "source": [
    "# 9) COMPLEX WORD COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "712fe0e8-49b9-4e99-9e41-24a150e8ef36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complex words count= 8\n"
     ]
    }
   ],
   "source": [
    "vowels=['a','e','i','o','u']\n",
    "import re\n",
    "count=0\n",
    "complex_Word_Count=0\n",
    "for i in tokenize_text:\n",
    "  x=re.compile('[es|ed]$')\n",
    "  if x.match(i.lower()):\n",
    "   count+=0\n",
    "  else:\n",
    "    for j in i:\n",
    "      if(j.lower() in vowels ):\n",
    "        count+=1\n",
    "  if(count>2):\n",
    "   complex_Word_Count+=1\n",
    "  count=0\n",
    "print('complex words count=',  complex_Word_Count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b417ead8-afb4-4bd8-a6ae-9db344ee0ad9",
   "metadata": {},
   "source": [
    "# 10) WORD COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "93afc95f-103a-4a85-b02e-54874e03207f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word count=  20\n"
     ]
    }
   ],
   "source": [
    "word_count=len(tokenize_text)\n",
    "print('word count= ', word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cabad38-8443-497b-8b53-bc13e449361e",
   "metadata": {},
   "source": [
    "# 11) SYLLABLE PER WORD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "66230f91-c1d2-4334-8581-ed9bb32e0a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "syllable_per_word=  47\n"
     ]
    }
   ],
   "source": [
    "vowels=['a','e','i','o','u']\n",
    "import re\n",
    "count=0\n",
    "for i in tokenize_text:\n",
    "  x=re.compile('[es|ed]$')\n",
    "  if x.match(i.lower()):\n",
    "   count+=0\n",
    "  else:\n",
    "    for j in i:\n",
    "      if(j.lower() in vowels ):\n",
    "        count+=1\n",
    "syllable_count=count\n",
    "print('syllable_per_word= ',syllable_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed2ba95-15a1-4e15-ba7b-340690ac5ca5",
   "metadata": {},
   "source": [
    "# 12) PERSONAL PRONOUNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "104de9c6-ddc9-46a0-b840-0fc73ddd0ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "personal pronouns=  0\n"
     ]
    }
   ],
   "source": [
    "pronouns=['i','we','my','ours','us' ]\n",
    "import re\n",
    "count=0\n",
    "for i in tokenize_text:\n",
    "  if i.lower() in pronouns:\n",
    "   count+=1\n",
    "personal_pronouns=count\n",
    "print('personal pronouns= ',personal_pronouns )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ce28a5-263e-4521-bf60-8e81aaebd073",
   "metadata": {},
   "source": [
    "# 13) AVG WORD LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "1df79273-f754-4969-a236-71bc5875de54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg word=  6.7\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for i in tokenize_text:\n",
    "  for j in i:\n",
    "    count+=1\n",
    "avg_word_length=count/len(tokenize_text)\n",
    "print('avg word= ', avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "331d48e6-81bd-400f-ab38-bdb61019ddb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data={'positive_score':positive_score,'negative_score':negative_score,'Polarity_Score':Polarity_Score,'subjectiivity_score':subjectiivity_score,'avg_senetence_length':avg_senetence_length,'Percentage_of_Complex_words':Percentage_of_Complex_words,'Fog_Index':Fog_Index,'avg_no_of_words_per_sentence':avg_no_of_words_per_sentence,'complex_Word_Count':complex_Word_Count,'word_count':word_count,'syllable_count':syllable_count,'personal_pronouns':personal_pronouns,'avg_word_length':avg_word_length}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "9a261fcd-801f-417c-bcfa-7963279ab618",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.read_excel('Output Data Structure.xlsx')\n",
    "\n",
    "# These are the required parameters \n",
    "variables = [positive_score,\n",
    "            negative_score,\n",
    "            Polarity_Score,\n",
    "            subjectiivity_score,\n",
    "            avg_senetence_length,\n",
    "            Percentage_of_Complex_words,\n",
    "            Fog_Index,\n",
    "            avg_no_of_words_per_sentence,\n",
    "            complex_Word_Count,\n",
    "            word_count,\n",
    "            syllable_count,\n",
    "            personal_pronouns,\n",
    "            avg_word_length]\n",
    "\n",
    "# write the values to the dataframe\n",
    "for i, var in enumerate(variables):\n",
    "  output_df.iloc[:,i+2] = var\n",
    "\n",
    "#now save the dataframe to the disk\n",
    "output_df.to_csv('Output_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecb5556-80a3-48d3-9cd2-b333580d36ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
